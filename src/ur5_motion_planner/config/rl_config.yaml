# RL Training Configuration for UR5 Environment

# Environment Parameters
environment:
  max_steps_per_episode: 200
  workspace_bounds:   # where the target will spawn
    x_min: -0.21
    x_max: 0.2
    y_min: -0.21
    y_max: 0.2
    z_min: 0.9
    z_max: 0.91
  goal_tolerance: 0.1  # distance tolerance for reaching the goal (current testing)
  action_scale: 1.0  # scale factor for velocity actions (increased for more visible movement)
  dt: 0.1  # time step for velocity integration (increased for more movement)

# Reward Parameters
reward:
  max_reward: 100.0
  distance_penalty_scale: 2.0
  goal_bonus: 2000.0

# Model Parameters
model:
  algorithm: "SAC"  # options: SAC (Soft Actor-Critic), ...
  policy: "MlpPolicy" # options: MlpPolicy, CnnPolicy, etc.
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  gradient_steps: 1
  ent_coef: "auto"
  target_update_interval: 1
  verbose: 1

# Training Parameters
training:
  total_timesteps: 2000
  log_interval: 4
  eval_episodes: 2
  tensorboard_log: "./ur5_sac_tensorboard/"
  model_save_path: "models/sac_ur5_reacher_model_cu.zip"

# Joint Limits (radians)
joint_limits:
  lower: [-3.14159, -3.14159, -3.14159, -3.14159, -3.14159, -3.14159]
  upper: [3.14159, 3.14159, 3.14159, 3.14159, 3.14159, 3.14159]

# Home Position
home_position: [0.0, -1.5708, 0.0, -1.5708, 0.0, 0.0]  # joint angles in radians

# ROS Parameters
ros:
  action_server_timeout: 20.0
  joint_state_timeout: 10.0
  trajectory_execution_time: 3.0  # increased for more time for robot to move
