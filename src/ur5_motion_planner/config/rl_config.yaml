# RL Training Configuration for UR5 Environment

# Environment Parameters
environment:
  max_steps_per_episode: 200
  workspace_bounds:   # where the target will spawn
    x_min: -0.21
    x_max: 0.2
    y_min: -0.21
    y_max: 0.2
    z_min: 0.9
    z_max: 0.91
  goal_tolerance: 0.05  # distance tolerance for reaching the goal
  action_scale: 0.5  # scale factor for velocity actions (reduced from 1.0)
  dt: 0.2  # time step for velocity integration (increased from 0.1)

# Reward Parameters
reward:
  max_reward: 100.0
  distance_penalty_scale: 500.0
  goal_bonus: 100.0

# Model Parameters
model:
  algorithm: "SAC"  # Soft Actor-Critic
  policy: "MlpPolicy"
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  gradient_steps: 1
  ent_coef: "auto"
  target_update_interval: 1
  verbose: 1

# Training Parameters
training:
  total_timesteps: 50000
  log_interval: 4
  eval_episodes: 10
  save_interval: 5000  # Save model every N timesteps
  tensorboard_log: "./ur5_sac_tensorboard/"
  model_save_path: "models/sac_ur5_reacher_model.zip"

# Joint Limits (radians)
joint_limits:
  lower: [-3.14159, -3.14159, -3.14159, -3.14159, -3.14159, -3.14159]
  upper: [3.14159, 3.14159, 3.14159, 3.14159, 3.14159, 3.14159]

# Home Position
home_position: [0.0, -1.5708, 0.01, -1.5708, 0.01, 0.01]  # Joint angles in radians

# ROS Parameters
ros:
  action_server_timeout: 20.0
  joint_state_timeout: 10.0
  trajectory_execution_time: 2.0  # increased from 1.0 to give robot more time to move
